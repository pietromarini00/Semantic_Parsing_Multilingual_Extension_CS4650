{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pietromarini00/Semantic_Parsing_Multilingual_Extension_CS4650/blob/main/CS4650_FinalProject_readable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjD5izOTitCv"
      },
      "source": [
        "# Part 1: Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "etXmRtCV-ng-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hn46p58iem1Q"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05yy9Lzd5BiW"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n",
        "from datasets import load_dataset, Split\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from functools import partial\n",
        "\n",
        "from torch import optim\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import time\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load pretrained model"
      ],
      "metadata": {
        "id": "jEX8hB9z-snr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F6q8_99eamZ"
      },
      "outputs": [],
      "source": [
        "sql_model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-wikiSQL\")\n",
        "sql_tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-wikiSQL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gGC-d8M3H6l"
      },
      "outputs": [],
      "source": [
        "sql_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tests with pretrained model"
      ],
      "metadata": {
        "id": "vkzieksj-zcr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFY3Hsv8gHPa"
      },
      "outputs": [],
      "source": [
        "#train_dataset  = load_dataset('wikisql', split=Split.TRAIN)\n",
        "#valid_dataset = load_dataset('wikisql', split=Split.VALIDATION)\n",
        "#test_dataset = load_dataset('wikisql', split=Split.TEST)\n",
        "#torch.save(train_dataset, \"train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBN5_jODkL-M"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# this is used for saving the english2sql\n",
        "print(train_dataset)\n",
        "import pandas as pd\n",
        "train = []\n",
        "for i in range(5000):\n",
        "  a = {}\n",
        "  a['question'] = train_dataset[i]['question']\n",
        "  a['sql'] = train_dataset[i]['sql']['human_readable']\n",
        "  train.append(a)\n",
        "print(train)\n",
        "import csv\n",
        "keys = train[0].keys()\n",
        "\n",
        "with open('train.csv', 'w', newline='') as output_file:\n",
        "    dict_writer = csv.DictWriter(output_file, keys, delimiter='|')\n",
        "    dict_writer.writeheader()\n",
        "    dict_writer.writerows(train)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "im5SJNEkeroh"
      },
      "outputs": [],
      "source": [
        "def get_sql(query):\n",
        "  input_text = \"translate English to SQL: %s </s>\" % query\n",
        "  features = sql_tokenizer([input_text], return_tensors='pt')\n",
        "\n",
        "  output = sql_model.generate(input_ids=features['input_ids'],\n",
        "               attention_mask=features['attention_mask'])\n",
        "\n",
        "  return sql_tokenizer.decode(output[0])\n",
        "\n",
        "query = \"What is the highest performance of model BERT?\"\n",
        "\n",
        "get_sql(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4ZOxASghGDD"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "for i in [0, 50, 100, 200, 500, 1000]:\n",
        "  print(\"Question:\", valid_dataset[i])\n",
        "  print(\"Answer:\", valid_dataset[i]['sql']['human_readable'])\n",
        "  print(\"Prediction:\", get_sql(valid_dataset[i]['question']))\n",
        "  print(\"============================================================================================================================\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwzS2nQ7i2wG"
      },
      "source": [
        "# First Model: Translate and Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-wTo8wVjiTQ"
      },
      "outputs": [],
      "source": [
        "translation_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\")\n",
        "translation_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL28Rjouj1rU"
      },
      "outputs": [],
      "source": [
        "def get_translation(text):\n",
        "  features = translation_tokenizer([text], return_tensors='pt')\n",
        "\n",
        "  output = translation_model.generate(input_ids=features['input_ids'],\n",
        "               attention_mask=features['attention_mask'])\n",
        "\n",
        "  return translation_tokenizer.decode(output[0])\n",
        "\n",
        "#query = \"What is the highest performance of a BERT model?\"\n",
        "new_query = \"Was ist die hoechste performance von einem model BERT\"\n",
        "\n",
        "get_translation(new_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3GS80onlh4p"
      },
      "outputs": [],
      "source": [
        "def get_german_sql(german_query):\n",
        "  features = translation_tokenizer([german_query], return_tensors='pt')\n",
        "\n",
        "  output = translation_model.generate(input_ids=features['input_ids'],\n",
        "               attention_mask=features['attention_mask'])\n",
        "\n",
        "  english_query = translation_tokenizer.decode(output[0])\n",
        "\n",
        "  #print(english_query)\n",
        "\n",
        "  input_text_english = \"translate English to SQL: %s </s>\" % english_query\n",
        "  #print(input_text_english)\n",
        "  features = sql_tokenizer([input_text_english], return_tensors='pt')\n",
        "\n",
        "  output = sql_model.generate(input_ids=features['input_ids'],\n",
        "               attention_mask=features['attention_mask'])\n",
        "\n",
        "  return sql_tokenizer.decode(output[0])\n",
        "\n",
        "get_german_sql(\"Auf welche Schule ist der Spieler gegangen, der von 2010-2012 in Toronto war?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFe4pa9ViqxQ"
      },
      "source": [
        "# Second Model: Retrain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DopGQrsljFQ6"
      },
      "source": [
        "## Setup Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "je3BCpyGixby"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "#load the data into a pandas dataframe\n",
        "full_df = pd.read_csv('train_german.csv', sep='; ', header=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bsCt5ILoIIg"
      },
      "outputs": [],
      "source": [
        "#divide data into train, validation, and test datasets\n",
        "num_queries = len(full_df)\n",
        "idxs = list(range(num_queries))\n",
        "print('Total queries in dataset: ', num_queries)\n",
        "test_idx = idxs[:int(0.1*num_queries)]\n",
        "val_idx = idxs[int(0.1*num_queries):int(0.2*num_queries)]\n",
        "train_idx = idxs[int(0.2*num_queries):]\n",
        "\n",
        "train_df = full_df.iloc[train_idx].reset_index(drop=True)\n",
        "val_df = full_df.iloc[val_idx].reset_index(drop=True)\n",
        "test_df = full_df.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "train_data = train_df[['question', 'sql']]\n",
        "val_data   = val_df[['question', 'sql']]\n",
        "test_data  = test_df[['question', 'sql']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNoSCFJppRNm"
      },
      "outputs": [],
      "source": [
        "#Defining torch dataset class for disaster tweet dataset\n",
        "class SQLDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.df.iloc[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l22oFjUDpTxD"
      },
      "outputs": [],
      "source": [
        "#set up train, validation, and testing datasets\n",
        "train_dataset = SQLDataset(train_data)\n",
        "val_dataset   = SQLDataset(val_data)\n",
        "test_dataset  = SQLDataset(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUG2JWx6jLiV"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPN_vSjQjN72"
      },
      "outputs": [],
      "source": [
        "sql_model_retrain = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-wikiSQL\")\n",
        "sql_tokenizer_retrain = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-wikiSQL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI4BJohoje2P"
      },
      "outputs": [],
      "source": [
        "#define hyperparameters\n",
        "BATCH_SIZE = 20\n",
        "LR = 1e-5\n",
        "WEIGHT_DECAY = 0\n",
        "N_EPOCHS = 5\n",
        "CLIP = 1.0\n",
        "\n",
        "#define models, move to device, and initialize weights\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = sql_model_retrain\n",
        "model.to(device)\n",
        "print('Model Initialized')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BdSjd4G0tgD"
      },
      "outputs": [],
      "source": [
        "def transformer_collate_fn(batch, tokenizer):\n",
        "\n",
        "  sentences, labels, masks = [], [], []\n",
        "  i = 0\n",
        "  for data in batch:\n",
        "    i += 1\n",
        "    #print(data['question'])\n",
        "    #print(sql_tokenizer([data['question']]))\n",
        "    #print(en_de_translator(data['question'])[0]['translation_text'])\n",
        "    tokenizer_output = sql_tokenizer([data['question']])\n",
        "    #print(tokenizer_output)\n",
        "    label_output = sql_tokenizer([data['sql']])\n",
        "    tokenized_sent = tokenizer_output['input_ids'][0]\n",
        "    label_sent = label_output['input_ids'][0]\n",
        "    mask = tokenizer_output['attention_mask'][0]\n",
        "    sentences.append(torch.tensor(tokenized_sent))\n",
        "    labels.append(torch.tensor(label_sent))\n",
        "    masks.append(torch.tensor(mask))\n",
        "  sentences = pad_sequence(sentences, batch_first=True, padding_value=0)\n",
        "  labels = pad_sequence(labels, batch_first=True, padding_value=0)\n",
        "  masks = pad_sequence(masks, batch_first=True, padding_value=0.0)\n",
        "  return sentences, labels, masks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load multilingual tokenizer\n",
        "from transformers import T5Tokenizer\n",
        "\n",
        "ml_tokenizer = T5Tokenizer.from_pretrained(\"google/mt5-large\")"
      ],
      "metadata": {
        "id": "QP_pnPpnnWGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1h33vj_kmBP"
      },
      "outputs": [],
      "source": [
        "#create pytorch dataloaders from train_dataset, val_dataset, and test_datset\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,collate_fn=partial(transformer_collate_fn, tokenizer=ml_tokenizer), shuffle = True)\n",
        "val_dataloader = DataLoader(val_dataset,batch_size=BATCH_SIZE,collate_fn=partial(transformer_collate_fn, tokenizer=ml_tokenizer))\n",
        "test_dataloader = DataLoader(test_dataset,batch_size=BATCH_SIZE,collate_fn=partial(transformer_collate_fn, tokenizer=ml_tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeCCeWGH0nqU"
      },
      "outputs": [],
      "source": [
        "#train a given model, using a pytorch dataloader, optimizer, and scheduler (if provided)\n",
        "def train(model,\n",
        "          dataloader,\n",
        "          optimizer,\n",
        "          device,\n",
        "          clip: float,\n",
        "          epoch,\n",
        "          scheduler = None):\n",
        "\n",
        "    model.train()\n",
        "    crit = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with tqdm.notebook.tqdm(\n",
        "                dataloader,\n",
        "                desc=\"epoch {}\".format(epoch + 1),\n",
        "                unit=\"batch\",\n",
        "                total=len(dataloader)) as batch_iterator:\n",
        "      for batch in batch_iterator:\n",
        "          sentences, labels, masks = batch[0], batch[1], batch[2]\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          output = model(input_ids=sentences.to(device), decoder_input_ids=labels.to(device))\n",
        "          loss = crit(output.logits.transpose(2, 1), labels.to(device))\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "          optimizer.step()\n",
        "          if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "          epoch_loss += loss.item()\n",
        "    return epoch_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqU7nJzy0Dj9"
      },
      "outputs": [],
      "source": [
        "# evaluate loss\n",
        "def evaluate(model,\n",
        "             dataloader,\n",
        "             device):\n",
        "\n",
        "    model.eval()\n",
        "    crit = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "      with tqdm.notebook.tqdm(\n",
        "                dataloader,\n",
        "                desc=\"Eval\",\n",
        "                unit=\"batch\",\n",
        "                total=len(dataloader)) as batch_iterator:\n",
        "        for batch in batch_iterator:\n",
        "            sentences, labels, masks = batch[0], batch[1], batch[2]\n",
        "            output = model(input_ids=sentences.to(device), decoder_input_ids=labels.to(device))\n",
        "            loss = crit(output.logits.transpose(2, 1), labels.to(device))\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTPg4uPl1Ohi"
      },
      "outputs": [],
      "source": [
        "#calculate the prediction accuracy on the provided dataloader\n",
        "def evaluate_acc(model,\n",
        "                 dataset,\n",
        "                 device):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "      total_correct = 0\n",
        "      position_correct = 0\n",
        "      total = 0\n",
        "      with tqdm.notebook.tqdm(\n",
        "                dataset,\n",
        "                desc=\"Eval Acc German2SQL\",\n",
        "                unit=\"batch\",\n",
        "                total=len(dataset)) as batch_iterator:\n",
        "        for batch in batch_iterator:\n",
        "\n",
        "            sentences, labels = batch[0], batch[1]\n",
        "            #print(sentences)\n",
        "            input_text = \"translate to SQL: %s </s>\" % sentences\n",
        "            features = sql_tokenizer_retrain([input_text], return_tensors='pt')\n",
        "            output = sql_model_retrain.generate(input_ids=features['input_ids'].to(device),\n",
        "                          attention_mask=features['attention_mask'].to(device))\n",
        "            output = sql_tokenizer_retrain.decode(output[0])\n",
        "            output_list = output.replace(\"</s>\", \"\").lower().split()[1:]\n",
        "            #print(output_list)\n",
        "            label_list = labels.lower().split()\n",
        "            #print(label_list)\n",
        "            for i, word in enumerate(label_list):\n",
        "              total += 1\n",
        "              if word in output_list:\n",
        "                total_correct += 1\n",
        "              try:\n",
        "                index = output_list.index(word)\n",
        "                if index == i:\n",
        "                  position_correct += 1\n",
        "              except:\n",
        "                pass\n",
        "\n",
        "    return total_correct / total#, position_correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YcukvE01A4X"
      },
      "outputs": [],
      "source": [
        "#count the number of trainable parameters in the model\n",
        "def count_parameters(model: torch.nn.Module):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "#computes the amount of time that a training epoch took and displays it in human readable form\n",
        "def epoch_time(start_time: int,\n",
        "               end_time: int):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i2kmKN_xDCs"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvTePmnCkrtq"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=10, num_training_steps=N_EPOCHS*len(train_dataloader))\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "\n",
        "\n",
        "# get example query\n",
        "query = \"Wie hoch ist die Gesamtzahl der Episoden, deren Erstausstrahlung von 1,82 Millionen Zuschauern gesehen wurde?\"\n",
        "input_text = \"translate to SQL: %s </s>\" % query\n",
        "features = sql_tokenizer_retrain([input_text], return_tensors='pt')\n",
        "output = sql_model_retrain.generate(input_ids=features['input_ids'].to(device),\n",
        "              attention_mask=features['attention_mask'].to(device))\n",
        "print(sql_tokenizer_retrain.decode(output[0]))\n",
        "\n",
        "train_loss = evaluate(model, train_dataloader, device)\n",
        "#train_acc = evaluate_acc(model, train_dataset, device)\n",
        "valid_loss = evaluate(model, val_dataloader, device)\n",
        "#valid_acc = evaluate_acc(model, val_dataset, device)\n",
        "\n",
        "print(f'Initial Train Loss: {train_loss:.8f}')\n",
        "#print(f'Initial Train Acc: {train_acc:.8f}')\n",
        "print(f'Initial Valid Loss: {valid_loss:.8f}')\n",
        "#print(f'Initial Valid Acc: {valid_acc:.8f}')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_dataloader, optimizer, device, CLIP, epoch, scheduler)\n",
        "    end_time = time.time()\n",
        "    #train_acc = evaluate_acc(model, train_dataset, device)\n",
        "    valid_loss = evaluate(model, val_dataloader, device)\n",
        "    #valid_acc = evaluate_acc(model, val_dataset, device)\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.8f}')\n",
        "    #print(f'\\tTrain Acc: {train_acc:.8f}')\n",
        "    print(f'\\tValid Loss: {valid_loss:.8f}')\n",
        "    #print(f'\\tValid Acc: {valid_acc:.8f}')\n",
        "    # get example query\n",
        "    features = sql_tokenizer_retrain([input_text], return_tensors='pt')\n",
        "    output = sql_model_retrain.generate(input_ids=features['input_ids'].to(device),\n",
        "                  attention_mask=features['attention_mask'].to(device))\n",
        "    print(sql_tokenizer_retrain.decode(output[0]))\n",
        "\n",
        "test_loss = evaluate(model, test_dataloader, device)\n",
        "#test_acc = evaluate_acc(model, test_dataset, device)\n",
        "print(f'Test Loss: {test_loss:.8f}')\n",
        "#print(f'\\tTest Acc: {test_acc:.8f}')\n",
        "# get example query\n",
        "features = sql_tokenizer_retrain([input_text], return_tensors='pt')\n",
        "output = sql_model_retrain.generate(input_ids=features['input_ids'].to(device),\n",
        "              attention_mask=features['attention_mask'].to(device))\n",
        "print(sql_tokenizer_retrain.decode(output[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ra9mZ2WSZoj"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYS7Ydz6Se6C"
      },
      "outputs": [],
      "source": [
        "# calculate Accuracy of German2English2SQL\n",
        "def evaluate_acc_tanslate_sql(dataloader,\n",
        "                 device):\n",
        "\n",
        "\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "      total_correct = 0\n",
        "      position_correct = 0\n",
        "      total = 0\n",
        "      with tqdm.notebook.tqdm(\n",
        "                dataloader,\n",
        "                desc=\"Eval Acc German2English2SQL\",\n",
        "                unit=\"batch\",\n",
        "                total=len(dataloader)) as batch_iterator:\n",
        "        for batch in batch_iterator:\n",
        "            sentences, labels = batch[0], batch[1]\n",
        "            #print(sentences)\n",
        "            output = get_german_sql(sentences)\n",
        "            output_list = output.replace(\"</s>\", \"\").lower().split()[1:]\n",
        "            # print(output_list)\n",
        "            label_list = labels.lower().split()\n",
        "            # print(label_list)\n",
        "            total_sentence = 0\n",
        "            total_sentence_correct = 0\n",
        "            for i, word in enumerate(label_list):\n",
        "              total += 1\n",
        "              total_sentence += 1\n",
        "              if word in output_list:\n",
        "                total_correct += 1\n",
        "                total_sentence_correct += 1\n",
        "              try:\n",
        "                index = output_list.index(word)\n",
        "                if index == i:\n",
        "                  position_correct += 1\n",
        "              except:\n",
        "                pass\n",
        "            # print(total_sentence_correct / total_sentence)\n",
        "\n",
        "    return total_correct / total, position_correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LG9QM0AUU5F"
      },
      "outputs": [],
      "source": [
        "evaluate_acc_tanslate_sql(test_dataset, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_Ia7o3EgsDb"
      },
      "outputs": [],
      "source": [
        "#calculate accuracy of German2SQL\n",
        "def evaluate_acc(model,\n",
        "                 dataset,\n",
        "                 device):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "      total_correct = 0\n",
        "      position_correct = 0\n",
        "      total = 0\n",
        "      with tqdm.notebook.tqdm(\n",
        "                dataset,\n",
        "                desc=\"Eval Acc German2SQL\",\n",
        "                unit=\"batch\",\n",
        "                total=len(dataset)) as batch_iterator:\n",
        "        for batch in batch_iterator:\n",
        "\n",
        "            sentences, labels = batch[0], batch[1]\n",
        "            #print(sentences)\n",
        "            input_text = \"translate to SQL: %s </s>\" % sentences\n",
        "            features = sql_tokenizer_retrain([input_text], return_tensors='pt')\n",
        "            output = sql_model_retrain.generate(input_ids=features['input_ids'].to(device),\n",
        "                          attention_mask=features['attention_mask'].to(device))\n",
        "            output = sql_tokenizer_retrain.decode(output[0])\n",
        "            output_list = output.replace(\"</s>\", \"\").lower().split()[1:]\n",
        "            # print(output_list)\n",
        "            label_list = labels.lower().split()\n",
        "            # print(label_list)\n",
        "            total_sentence = 0\n",
        "            total_sentence_correct = 0\n",
        "            for i, word in enumerate(label_list):\n",
        "              total += 1\n",
        "              total_sentence += 1\n",
        "              if word in output_list:\n",
        "                total_correct += 1\n",
        "                total_sentence_correct += 1\n",
        "              try:\n",
        "                index = output_list.index(word)\n",
        "                if index == i:\n",
        "                  position_correct += 1\n",
        "              except:\n",
        "                pass\n",
        "            # print(total_sentence_correct / total_sentence)\n",
        "\n",
        "    return total_correct / total, position_correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch8tgZ3OYFhX"
      },
      "outputs": [],
      "source": [
        "evaluate_acc(sql_model_retrain, test_dataset, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}